{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Capstone.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NNdghp0rfvQd",
        "OG_UWsozhs3i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3N1FZLzbfzVU"
      },
      "source": [
        "#### Capstone Project: Deep Learning and Quantum Information Theory\n",
        "\n",
        "### Quantum Code Generation with Conditional RNNs\n",
        "_Yoav Rabinovich, March 2020_\n",
        "\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NNdghp0rfvQd"
      },
      "source": [
        "#### Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOZaX7qNKHD8",
        "colab": {}
      },
      "source": [
        "!pip install qiskit\n",
        "!pip install tensorflow-gpu --upgrade\n",
        "!pip install -Iv cond-rnn==1.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J_ycOxIXKsKk",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import qiskit as qk\n",
        "import tensorflow.compat.v1 as tf\n",
        "import cond_rnn as crnn\n",
        "import re\n",
        "import os\n",
        "from google.colab import output\n",
        "tf.disable_v2_behavior()\n",
        "qs_backend = qk.Aer.get_backend('statevector_simulator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OG_UWsozhs3i"
      },
      "source": [
        "#### Circuit Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DPgC4Wa8Ky4K",
        "colab": {}
      },
      "source": [
        "def sample_circuits(n,size,amount):\n",
        "    \"\"\"Sample an amount of random n-qubit circuits with a certain size in\n",
        "    number of operations from the allowed set\"\"\"\n",
        "\n",
        "    circuits =[]\n",
        "    for _ in range(amount):\n",
        "        # Create circuit object of n qubits\n",
        "        circ = qk.QuantumCircuit(n)\n",
        "        # Generate random gates on random qubits from the universal set {H,S,CX}\n",
        "        for _ in range(size):\n",
        "            gate = np.random.randint(0,3)\n",
        "            target = np.random.randint(0,n)\n",
        "            if gate==0: # Hadamard\n",
        "                circ.h(target)\n",
        "            if gate==1: # S-gate\n",
        "                circ.s(target)\n",
        "            if gate==2: # CNOT\n",
        "                control = np.random.randint(0,n)\n",
        "                if control == target:\n",
        "                    circ.h(target)\n",
        "                else:\n",
        "                    circ.cx(control,target)\n",
        "        circuits.append(circ)\n",
        "    return circuits\n",
        "\n",
        "def encode_circuits(circuits,n,max_size,label=True):\n",
        "    \"\"\"Takes an array of n-qubit QuantumCircuit objects, and encodes them based on a\n",
        "    vocabulary of possible gates to apply, including tokens to signify the start\n",
        "    and end of sequences. Elements after EoS are padded to match maximum circuit\n",
        "    size using a special token.\n",
        "    Labels can be also be generated for the circuits.\"\n",
        "\n",
        "    Vocabulary scheme:\n",
        "    Padding = 0,\n",
        "    SoS = 1,\n",
        "    EoS = 2,\n",
        "    h[0]=3, h[1]=3+1...\n",
        "    s[0]=3+n, s[1]=3+n+1...\n",
        "    cx[0,0]=3+2n, cx[0,1]=3+2n+1...\n",
        "    cx[1,0]=3+(2+1)n, cx[1,1]=3+(2+1)n+1... etc. \"\"\"\n",
        "\n",
        "    encoded = []\n",
        "    for circ in circuits:\n",
        "        # Use the QASM format to convert the circuit to a string\n",
        "        lines = circ.qasm().splitlines()[3:]\n",
        "        size = len(lines)\n",
        "        # Initialize to padding tokens\n",
        "        encoded_circ = np.zeros(max_size+2)\n",
        "        # Add SoS and EoS tokens\n",
        "        encoded_circ[0] = 1\n",
        "        encoded_circ[size+1]=2\n",
        "        for i,line in enumerate(lines):\n",
        "            # Detect gate name and qubits involved\n",
        "            gate_str = line[:2]\n",
        "            integers = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*',line)]\n",
        "            # Encode gates based on scheme above\n",
        "            if gate_str==\"h \":\n",
        "                encoded_circ[i+1]=int(3+integers[0])\n",
        "            if gate_str==\"s \":\n",
        "                encoded_circ[i+1]=int(3+n+integers[0])\n",
        "            if gate_str==\"cx\":\n",
        "                encoded_circ[i+1]=int(3+(2+integers[0])*n+integers[1])\n",
        "        encoded.append(encoded_circ)\n",
        "    encoded = np.array(encoded)\n",
        "    if label:\n",
        "        # Simulate labels for each circuit and attach to dataset\n",
        "        labels = generate_labels(circuits)\n",
        "        return np.concatenate((encoded,labels),axis=1)\n",
        "    else:\n",
        "        return np.array(encoded)\n",
        "\n",
        "def decode_circuit(encoded,n,debug=False):\n",
        "    \"\"\"Takes an encoded output from the network and generates the corresponding\n",
        "    circuit as described above.\"\"\"\n",
        "\n",
        "    # Start with opening syntax\n",
        "    decoded = \"OPENQASM 2.0;\\ninclude \\\"qelib1.inc\\\";\\nqreg q[\"+str(n)+\"];\\n\"\n",
        "    for line in encoded:\n",
        "        # decode each non-token element into its QASM string\n",
        "        line = int(np.around(line))\n",
        "        if debug: print(line)\n",
        "        if line > 2:\n",
        "            gate_num = int(np.ceil((line-2)/n))\n",
        "            if debug: print(gate_num)\n",
        "            if gate_num==1:\n",
        "                decoded += \"h q[\"+str(line-3)+\"];\\n\"\n",
        "            elif gate_num==2:\n",
        "                decoded += \"s q[\"+str(line-n-3)+\"];\\n\"\n",
        "            elif (str(gate_num-3)!=str(line-(gate_num-1)*n-3)):\n",
        "                decoded += \"cx q[\"+str(gate_num-3)+\"],q[\"+str(line-(gate_num-1)*n-3)+\"];\\n\"\n",
        "        if line == 2:\n",
        "            decoded = decoded[:-1]\n",
        "            break\n",
        "        if debug: print(decoded)\n",
        "    if debug:\n",
        "        print(decoded)\n",
        "    # Build circuit object from QASM string\n",
        "    return qk.QuantumCircuit.from_qasm_str(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM8Z_gH3RERT",
        "colab_type": "text"
      },
      "source": [
        "#### Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8pfInzSB_H",
        "colab_type": "text"
      },
      "source": [
        "##### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jufgi7NEbyNn",
        "colab": {}
      },
      "source": [
        "# Model Parameters\n",
        "N = 5 # size of qubit register\n",
        "MAX_LENGTH = 10 # maximum length of gate sequence\n",
        "VOCAB_DIM = 2+2*N+N**2 # amount of possible gates\n",
        "NUM_SAMPLES = 10000 # number of circuits in the database\n",
        "TRAINTEST = 0.75 # proportion of training data\n",
        "CUTOFF = int(NUM_SAMPLES//2)#(1/TRAINTEST))\n",
        "TIME_STEPS = MAX_LENGTH+2 # length of sequence including tokens\n",
        "INPUT_DIM = 1 # size of input element\n",
        "LABEL_DIM = 1 # size of label element\n",
        "COND_DIM = 2*(2**N) # length of condition state\n",
        "NUM_CELLS = 256 # number of cells in each RNN layer\n",
        "STACK_DEPTH = 5 # number of stacked RNN layers\n",
        "NUM_DENSE = 64 # size of dense classification layers\n",
        "CLASS_DEPTH = 5 # number of stacked classification layers\n",
        "LOSS_DELAY = 10 # number of epochs between each loss recording\n",
        "PRINT_DELAY = 10 # number of recordings between each loss printing\n",
        "SAVE_DELAY = 10000 # number of epochs between each model checkpoint\n",
        "\n",
        "# Training Parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1000 # hard cutoff for training\n",
        "\n",
        "# Model Structure\n",
        "ONE_HOT = True\n",
        "INPUT_CONCATENATED = False\n",
        "\n",
        "# Correct input dimensions\n",
        "if ONE_HOT:\n",
        "    INPUT_DIM = VOCAB_DIM\n",
        "    LABEL_DIM = VOCAB_DIM\n",
        "\n",
        "if INPUT_CONCATENATED:\n",
        "    INPUT_DIM += COND_DIM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxmKdIl-RnBO",
        "colab_type": "text"
      },
      "source": [
        "##### Generate Data (Option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSZp3OeYRzEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Data\n",
        "# Data = encode_circuits(sample_circuits(N,MAX_LENGTH,NUM_SAMPLES),N,MAX_LENGTH,label=True)\n",
        "# Save Data\n",
        "# np.savetxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", Data, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFWzZKD6Rtqo",
        "colab_type": "text"
      },
      "source": [
        "##### Load Data (Option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cfmMroOR2a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Data\n",
        "Data = np.loadtxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O8U69reTU4X",
        "colab_type": "text"
      },
      "source": [
        "##### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8C-Q6inddm5o",
        "colab": {}
      },
      "source": [
        "# Shuffle dataset\n",
        "np.random.shuffle(Data)\n",
        "\n",
        "# Inputs\n",
        "X = Data[:,:TIME_STEPS]\n",
        "X = X.reshape((X.shape[0],X.shape[1],1))\n",
        "if ONE_HOT:\n",
        "    # Convert to One-Hot Encoding\n",
        "    eye = np.eye(VOCAB_DIM)\n",
        "    X = np.array([[eye[int(gate)] for gate in X[int(row),:]] for row in range(X.shape[0])])\n",
        "\n",
        "# Labels\n",
        "offset = X[:,1:]\n",
        "y = np.concatenate((offset,np.zeros((X.shape[0],1,LABEL_DIM))),axis=1)\n",
        "\n",
        "# Conditions\n",
        "c = Data[:,TIME_STEPS:]\n",
        "\n",
        "# Split\n",
        "X_train = X[:CUTOFF]\n",
        "X_test = X[CUTOFF:]\n",
        "c_test = c[:CUTOFF]\n",
        "c_train = c[CUTOFF:]\n",
        "y_train = y[:CUTOFF]\n",
        "y_test = y[CUTOFF:]\n",
        "\n",
        "if INPUT_CONCATENATED:\n",
        "    # Append conditions to inputs\n",
        "    c_train_dup = np.repeat(c_train[:, :, np.newaxis],TIME_STEPS,axis=2)\n",
        "    c_train_rot = np.rot90(c_train_dup,1,(1,2))\n",
        "    X_train = np.concatenate((X_train,c_train_rot),axis=2)\n",
        "\n",
        "    c_test_dup = np.repeat(c_test[:, :, np.newaxis],TIME_STEPS,axis=2)\n",
        "    c_test_rot = np.rot90(c_test_dup,1,(1,2))\n",
        "    X_test = np.concatenate((X_test,c_test_rot),axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3cz7V7bUPvx",
        "colab_type": "text"
      },
      "source": [
        "#### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-6jX8x5_c7eA",
        "colab": {}
      },
      "source": [
        "# Define Session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Placeholders for variables\n",
        "inputs = tf.placeholder(name='inputs', dtype=tf.float32, shape=(None, TIME_STEPS, INPUT_DIM))\n",
        "targets = tf.placeholder(name='targets', dtype=tf.float32, shape=(None, TIME_STEPS, LABEL_DIM))\n",
        "if not INPUT_CONCATENATED:\n",
        "    # Condition variables for hidden-state model\n",
        "    cond = tf.placeholder(name='conditions', dtype=tf.float32, shape=(None, COND_DIM))\n",
        "\n",
        "# Conditional RNN\n",
        "if INPUT_CONCATENATED:\n",
        "    # Regular GRU\n",
        "    outputs = tf.keras.layers.GRU(NUM_CELLS, return_sequences=True)(inputs)\n",
        "    for _ in range(STACK_DEPTH-1):\n",
        "        outputs = tf.keras.layers.GRU(NUM_CELLS, return_sequences=True)(outputs)\n",
        "else:\n",
        "    # Conditional GRU\n",
        "    outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "    for _ in range(STACK_DEPTH-1):\n",
        "        outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(outputs)\n",
        "\n",
        "# Classification\n",
        "for _ in range(CLASS_DEPTH):\n",
        "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=NUM_DENSE, activation='relu'))(outputs)\n",
        "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=LABEL_DIM, activation='relu'))(outputs)\n",
        "\n",
        "# Loss + Optimizer\n",
        "if ONE_HOT:\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets))\n",
        "else:\n",
        "    cost = tf.reduce_sum(tf.reduce_mean(tf.squared_difference(outputs, targets)))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "# Initialize variables (tensorflow)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Define the binding between placeholders and real data.\n",
        "if INPUT_CONCATENATED:\n",
        "    train_feed_dict = {inputs: X_train, targets: y_train}\n",
        "    test_feed_dict = {inputs: X_test, targets: y_test}\n",
        "else:\n",
        "    train_feed_dict = {inputs: X_train, targets: y_train, cond: c_train}\n",
        "    test_feed_dict = {inputs: X_test, targets: y_test, cond: c_test}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoAx4gw2T3k6",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5v7CoDbby89K",
        "colab": {}
      },
      "source": [
        "# Main loop. Optimize then evaluate.\n",
        "saver = tf.train.Saver()\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    sess.run(optimizer, train_feed_dict)\n",
        "    if epoch % LOSS_DELAY == 0:\n",
        "        train_outputs, train_loss = sess.run([outputs, cost], train_feed_dict)\n",
        "        test_outputs, test_loss = sess.run([outputs, cost], test_feed_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "    if epoch % (PRINT_DELAY*LOSS_DELAY) == 0:\n",
        "        print(f'[{str(epoch).zfill(4)}] train cost = {train_loss:.4f}, test cost = {test_loss:.4f}.')\n",
        "    if epoch % SAVE_DELAY == 0:\n",
        "        saver.save(sess, f'Checkpoints/QCG-{str(NUM_CELLS)}', global_step=int(epoch/SAVE_DELAY))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bG_jc0_UUId",
        "colab_type": "text"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ujtxMM5Anyyt",
        "colab": {}
      },
      "source": [
        "# Plot Losses\n",
        "epochs = np.arange(0,epoch,PRINT_DELAY)\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "plt.plot(epochs,train_losses,c=\"blue\",label=\"Train Loss\")\n",
        "plt.plot(epochs,test_losses,c=\"orange\",label=\"Test Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7loLgREMfba0",
        "colab": {}
      },
      "source": [
        "# Forward pass examples\n",
        "test_examples, test_example_loss = sess.run([outputs, cost], test_feed_dict)\n",
        "train_examples, train_example_loss = sess.run([outputs, cost], train_feed_dict)\n",
        "\n",
        "if ONE_HOT:\n",
        "    # OHE to Vocab\n",
        "    ones_test = np.full((test_examples.shape[0],1),1)\n",
        "    ones_train = np.full((train_examples.shape[0],1),1)\n",
        "    ones_y_test = np.full((y_test.shape[0],1),1)\n",
        "    ones_y_train = np.full((y_train.shape[0],1),1)\n",
        "    vocab_test = np.argmax(np.around(test_examples),2)[:,:-1]\n",
        "    vocab_train = np.argmax(np.around(train_examples),2)[:,:-1]\n",
        "    vocab_y_test = np.argmax(np.around(y_test),2)[:,:-1]\n",
        "    vocab_y_train = np.argmax(np.around(y_train),2)[:,:-1]\n",
        "    test_examples = np.concatenate((ones_test,vocab_test),1)\n",
        "    train_examples = np.concatenate((ones_train,vocab_train),1)\n",
        "    test_y_examples = np.concatenate((ones_y_test,vocab_y_test),1)\n",
        "    train_y_examples = np.concatenate((ones_y_train,vocab_y_train),1)\n",
        "else:\n",
        "    test_y_examples =  np.copy(y_test)\n",
        "    train_y_examples = np.copy(y_train)\n",
        "\n",
        "# Decode examples and simulate\n",
        "decoded_test = []\n",
        "decoded_train = []\n",
        "for i in range(train_examples.shape[0]):\n",
        "    if i<=test_examples.shape[0]:\n",
        "        encoded_test = np.concatenate((np.array([1]),test_examples[i].flatten()[:-2],np.array([2])))\n",
        "        decoded_test.append(decode_circuit(encoded_test,N))\n",
        "    encoded_train = np.concatenate((np.array([1]),train_examples[i].flatten()[:-2],np.array([2])))\n",
        "    decoded_train.append(decode_circuit(encoded_train,N))\n",
        "test_labels = generate_labels(decoded_test)\n",
        "train_labels = generate_labels(decoded_train)\n",
        "\n",
        "# Compute Metrics\n",
        "test_sequence_loss=np.mean(np.mean(test_examples!=test_y_examples,axis=1))\n",
        "test_target_loss=np.mean(np.mean((c_test-test_labels)**2,axis=1))\n",
        "train_sequence_loss=np.mean(np.mean( train_examples!=train_y_examples,axis=1))\n",
        "train_target_loss=np.mean(np.mean((c_train-train_labels)**2,axis=1))\n",
        "\n",
        "# Print\n",
        "print(\"Sequence Loss (test/train):\",test_sequence_loss,\"/\",train_sequence_loss)\n",
        "print(\"Target Loss   (test/train):\",test_target_loss,\"/\",train_target_loss)\n",
        "print(\"RNN Loss      (test/train):\",test_example_loss,\"/\",train_example_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuD-9uOTflzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Demonstrate example\n",
        "i=np.random.randint(test_examples.shape[0])\n",
        "print(np.around(test_examples[i]).T)\n",
        "print(test_y_examples[i].T)\n",
        "print(test_labels[i])\n",
        "print(c_test[i])\n",
        "decoded_test[i].draw()\n",
        "\n",
        "# Draw histogram of predicted gates\n",
        "# plt.hist(np.around(test_examples.flatten()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN4UsW4gC3A-",
        "colab_type": "text"
      },
      "source": [
        "##### Audio Notification of Completion (Option)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unSzufs_gPGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu2qFSNyEEbC",
        "colab_type": "text"
      },
      "source": [
        "#### _Yoav Rabinovich, March 2020_\n",
        "\n",
        "--------------------------------"
      ]
    }
  ]
}